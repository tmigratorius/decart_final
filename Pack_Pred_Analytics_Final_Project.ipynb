{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic for notebook assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model development, Validation, Testing Supervised learnig exercize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. using the sklearn iris dataset, build models using 3 different methods\n",
    "    - SVC\n",
    "    - KNN\n",
    "    - \n",
    "2. Inspect data/preprocess\n",
    "4. Split data into training/validation/test buckets\n",
    "5. use sklearn to develop models\n",
    "7. pick best model, run on test data\n",
    "8. Interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "#split the data twice to get train, validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "iris.data.shape, iris.target.shape\n",
    "X_train.shape, y_train.shape\n",
    "X_val.shape, y_val.shape\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear 0.9583333333333334\n",
      "poly 0.9166666666666666\n",
      "rbf 1.0\n"
     ]
    }
   ],
   "source": [
    "#this fits a linear support vector machine to the data\n",
    "#trying 3 different kernels\n",
    "classify_lin = svm.SVC(kernel='linear').fit(X_train, y_train)\n",
    "classify_poly = svm.SVC(kernel='poly',degree=3).fit(X_train, y_train)\n",
    "classify_rad = svm.SVC(kernel='rbf').fit(X_train, y_train)\n",
    "\n",
    "#test each with the set aside test data\n",
    "for kernels in [classify_lin, classify_poly, classify_rad]:\n",
    "    print(kernels.kernel, kernels.score(X_val, y_val))\n",
    "    scores.update( {kernels.kernel : kernels.score(X_val, y_val)} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn import neighbors as knn\n",
    "#number of neighbors to use\n",
    "n_neighbors = 10\n",
    "#try both uniform and distance weights\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    knn_classify = knn.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    knn_classify.fit(X_train, y_train)\n",
    "#see how each model works on the test data  \n",
    "    knn_classify.score(X_val,y_val)\n",
    "    \n",
    "    scores.update({weights:knn_classify.score(X_val,y_val)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_classify = RandomForestClassifier(n_estimators=10).fit(X_train,y_train)\n",
    "#score on the validation set\n",
    "forest_classify.score(X_val,y_val)\n",
    "\n",
    "scores.update({'forest':forest_classify.score(X_val,y_val)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distance': 0.9583333333333334,\n",
       " 'forest': 0.9583333333333334,\n",
       " 'linear': 0.9583333333333334,\n",
       " 'poly': 0.9166666666666666,\n",
       " 'rbf': 1.0,\n",
       " 'uniform': 1.0}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at all the scores and pick the best model\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC.rbf and KNN.uniform have the highest scores. Pick SVC\n",
    "# evaluate test data with that model\n",
    "classify_rad.score(X_test,y_test)\n",
    "\n",
    "#on the test data, the model has a mean accuracy of 96.67%\n",
    "#good performance on this simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#references\n",
    "# http://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily class summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 1\n",
    "- Introduction and challenges\n",
    "\n",
    "Talked about a few good and bad examples of real world machine learning and predictive analytics applications (watson health, amazon facial recognition, etc. Much of prediction and machine learning is still very experimental \n",
    "Went over the types of machine learning, supervised, unsupervised, reinforcement etc and the attributes of each. Tied these examples to various medical applications. Discussed various challenges associated with medical data. \n",
    "\n",
    "- Quick review of the pandas package for data science\n",
    "\n",
    "Discussed basics of python. Spent some time learning about and using basic functions of the Pandas package. Used imported data to set up linear regression models\n",
    "\n",
    "- Introduction to Regression with Linear Models\n",
    "\n",
    "Went over the basics of linear modeling, what the equation variables and coefficients represent, and how our data is structured influences what type of analysis we can do. Applied regressions to training and then test data and evaluated the performance of the models. Ran several iterations adding different or additional predictor variables each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 2\n",
    "- Bias, Regularization, and Variance\n",
    "\n",
    "we talked about how simple models are more biased, but have less variance, and more flexible models are less biased but have more variance. Discussed various types of regularization and how there is no one size fits all solution to machine learning. Did a few exercizes using scikit-learn for normalizing and preprocessing data and redid our initial regression with normalized data\n",
    "\n",
    "- Bayes\n",
    "\n",
    "Discussed the theory behind Bayesian statistics and went through a multiple regression example using radon data. Discussed reasons why results may not be expected, how to refine and tweak the model, and how to deal with uncertainty\n",
    "\n",
    "- Unsupervined Learning\n",
    "\n",
    "Talked about the different types of clustering algorithms and how they differ. Used radon data for cluster analysis, experimented with different nubmers of clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Day 3\n",
    "\n",
    "- Introduction\n",
    "\n",
    "Disscussed supervised learning, labeled and unlabeled data and the difference between classification and predictions. Went over the concepts of bias and variance, precision and accuracy, and many other metrics that determine the preformance of a model\n",
    "\n",
    "- MIMIC example\n",
    "\n",
    "Accessed tabels from the MIMIC II database and discussed how to organize and reshuffle the data to fit the form we need for our analysis. Began exploring the data and how to properly access tabels that can be millions of rows. Went over ploting and displaying aspects of the data in order to understand which variables may need to be dropped, wether or not to restrict variables to a range.\n",
    "\n",
    "- Classifiers\n",
    "\n",
    "Explained classifiers and logistic regression. Explained the effects/similarities/differences between standardization and normalization. Generated several simple logistic models changing input data to demonstrate how simple models may not be sensative enough to distinguish or vary with input data. Models are paramaters\n",
    "\n",
    "- Support Vector Machines\n",
    "\n",
    "Talked about hard and soft margin svms and how to deal with under and overfitting the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 4\n",
    "- Introduction \n",
    "\n",
    "Began with a review of bootstrapping and cross-validation, talked about what situations make it a better idea to choose one over another. Began calculating simple examples for connected nets and backpropogation. Explained the need for several layers, linear and non-linear.\n",
    "\n",
    "- Deep Learning\n",
    "\n",
    "Explained terminology and basics of deep learning explained tensors and the packages/methods used to develop networks. Ran through an example with the mnist database building the model and examining the results. We ran several more examples on IMDB and Reuters databases, examining how changes to layers changed over/underfitting of the model. \n",
    "Went over the typical steps of deep learning: split, train, validate, finailaze, test\n",
    "\n",
    "- Convolutional Neural Network\n",
    "\n",
    "Viewed animated examples of CNNs and filters. Discussed terminology and background information on CNNs. Ran examples on the mnist fashion database of clothing images-discussed the different layers and their function and how to tweak or tune them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Lunchtime Talk Summary\n",
    "#### Speaker: Eric Gibbons, PhD\n",
    "#### Title: Deep learning applications for MRI data acquisition and reconstruction\n",
    "bleetn\n",
    "- MRI scans\n",
    "- #1 improvement in MRI experience was intro of movies\n",
    "\n",
    "- MRI physics\n",
    "    - MRIs rely on strong magnetic fields to align body atoms, signals from spins is detected and translated into an image. The signal acquired is in the Fourier domain, taking the inverse Fourier transform gives the image\n",
    "    - Because each case has to be analyzed and measured individually, it's very slow. There has been a great deal of work to speed up MRI time by reducing sampling points, but this leads to issues\n",
    "    - though there are issues, MRI gives great soft tissue contrast\n",
    "- MRIs and Deep Learning\n",
    "    - More flexibility, better images, and better patient satisfaction\n",
    "    - Classification networks were initially used to improve the process\n",
    "    - Regression networks are a newer way to improve MRIs\n",
    "        - this changes the loss function, changes the problem to a regression problem to minimize the loss\n",
    "- Examples from the MRI field\n",
    "\n",
    "    - Knee Super Resolution\n",
    "     \n",
    "    the problem is to take a poor, pixelated image and produce something sharper and clearer, traditional algorithms are slow and the problem is ill-defined. Taking a deep learning approach may have a high time requirement in the training phase but end processing is quick\n",
    "    \n",
    "    The CNN was trained on pairs of images (high res, low res), taught the CNN to calculate the residual. Problem tries to minimize the difference between the true residual and the modeled residual. As with ALL data science, the available data is the real issue. Compared several interpolation strategies (tricubic interpolation, Fourier interpolation, DeepResolve-the CNN). The CNN did tend to better preserve pathologies in the images\n",
    "    \n",
    "    - Cardiac perfusion MRI temporal inetropolation\n",
    "    \n",
    "    Heart imaging is difficult as the heart is constantly moving. Contrast material is used to highlight diseased areas. Taking multiple slices at the same time is hard, traditional algorithms cannot capture the desired number of slices. This project took images at different times and then merged the images to interpolate intermediate states. This doubled the number of slices captured each beat. Deep learning has a much better time interpolating the optical flow. This network 'Separable convolution interpolation' (U-net architecture), and calculates the convolutional kernel per pixel. Because each pixel is evaluated, it is very customizable/extensible to many applications. The original training data was youtube videos, and SepConv was then fine-tuned on medical images. Training from scratch needs a large training dataset\n",
    "    \n",
    "    The SepConv approach also better perserved pathologies in the images, produced fewer artifacts, and had less residual.\n",
    "    \n",
    "    - Accelerating Stroke Imaging\n",
    "    \n",
    "    Diffusion contrast better reflects the physiology of imaged area. Diffusion weighted contrast has a much higher image slice requirement. Estimation techniques are used to reduce imaging time, but a great deal of detail is lost. Image generation is also very slow. \n",
    "    \n",
    "    The proposed deep learning approach is to subsample, run a reduced number of images through a convolutional neural network. Undersampling and processing with the CNN, the scan could be reduced from 12min to 3 min with minimal detail loss. The 2D deep learning approach preserves much of the detail and structure and is significantly better than the 1D method. Simultaneous training also greatly improved results, as the simultaneous parameter maps were self-reinforcing.\n",
    "    \n",
    "    \n",
    "Deep Learning for MRI and other signal processing is an expanding field and promises to produce more innovations and breakthroughs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
